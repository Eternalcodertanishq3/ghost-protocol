"""
Module: sna/main.py
DPDP ยง: 7(1) - Sovereignty (NIC Cloud India), ยง8(2)(a) - Data residency
Description: Secure National Aggregator (SNA) for Ghost Protocol
API: POST /aggregate, GET /global_model, GET /status, GET /leaderboard
"""

import asyncio
import logging
import signal
import sys
from typing import Dict, Any, List, Optional
from datetime import datetime
import json
import os
import threading
import time
import numpy as np

import torch
import torch.nn as nn
from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

# SNA components
from .byzantine_shield import ByzantineShield, HospitalUpdate
from .health_ledger import HealthLedger
from .dpdp_auditor import DPDPAuditor
from .sharding import HierarchicalAggregator
from .resilient_cache import ResilientCache  # Fix #2: Circuit breaker cache
from .bounded_queue import BoundedUpdateQueue  # Fix #5: Bounded queue
from .persistence import PersistenceManager  # Fix #13: DB persistence
from .health_check import HealthChecker, HealthStatus, check_redis, check_postgres  # Fix #10
from .rate_limited_broadcast import RateLimitedBroadcaster, MessagePriority  # Fix #12
from .api_models import (  # Fix #8: Pydantic validation
    HospitalUpdateRequest,
    HospitalUpdateResponse,
    SystemStatusResponse,
    HealthCheckResponse,
    LeaderboardResponse,
    LeaderboardEntry,
    GlobalModelResponse,
    ErrorResponse,
    WeightTensor
)

# Shared model registry - Fix #4: Single source of truth
from models.registry import ModelRegistry, DiabetesPredictionModel

# Configuration
from config import config, SECURITY_THRESHOLDS

# Use model from registry instead of duplicate definition (Fix #4)
# The DiabetesPredictionModel is now imported from models.registry
# Alias for backward compatibility
SimpleNN = DiabetesPredictionModel



class SecureNationalAggregator:
    """
    Secure National Aggregator (SNA) for Ghost Protocol.
    
    Implements:
    - Byzantine-robust model aggregation
    - HealthToken reward distribution
    - DPDP compliance monitoring
    - Global model management
    - Real-time hospital monitoring
    """
    
    def __init__(
        self,
        sna_port: int = 8000,
        redis_url: str = "redis://localhost:6379",
        vault_addr: str = "http://localhost:8200"
    ):
        """
        Initialize Secure National Aggregator.
        
        Args:
            sna_port: Port for SNA API
            redis_url: Redis URL for caching
            vault_addr: Vault address for secrets
        """
        self.sna_port = sna_port
        self.redis_url = redis_url
        self.vault_addr = vault_addr
        
        # Initialize resilient cache (Fix #2: Circuit breaker pattern)
        self.cache = ResilientCache(
            redis_url=redis_url,
            max_memory_items=10000,
            default_ttl=3600
        )
        
        # Initialize persistence manager (Fix #13: DB persistence)
        self.persistence = PersistenceManager(
            database_url=os.getenv(
                "POSTGRES_URL",
                "postgresql+asyncpg://ghost_user:ghost_password@localhost:5432/ghost_protocol"
            )
        )
        
        # Shutdown control
        self.is_shutting_down = False
        self.accepting_updates = True
        
        # Initialize components
        self.byzantine_shield = ByzantineShield(
            z_score_threshold=SECURITY_THRESHOLDS["z_score_anomaly"],
            reputation_decay_factor=config.REPUTATION_DECAY
        )
        
        self.health_ledger = HealthLedger(
            base_reward_tokens=10
        )
        self.shard_aggregator = HierarchicalAggregator(shard_size=10) # Using small shard size for demo scalability
        
        self.dpdp_auditor = DPDPAuditor(
            max_epsilon=config.MAX_EPSILON,
            max_delta=config.DELTA,
            auto_halt_enabled=True
        )
        
        # Global model - matches Ghost Agent DiabetesPredictionModel architecture
        self.global_model = DiabetesPredictionModel(
            input_size=8,   # Diabetes prediction features
            hidden_size=64  # Matches Agent's hidden size
        )
        
        # Training state
        self.current_round = 0
        self.global_weights = self._extract_model_weights()
        self.pending_updates: List[HospitalUpdate] = []
        self.is_aggregating = False
        
        # Performance tracking
        self.model_performance = 0.0
        self.round_history = []
        
        # WebSocket connections for real-time updates
        self.active_connections: List[WebSocket] = []
        
        # Initialize FastAPI app
        self.app = FastAPI(
            title="Ghost Protocol - Secure National Aggregator",
            description="DPDP-Safe Federated Learning Aggregator",
            version="1.0.0"
        )
        
        # Configure CORS - Fix #12: Use whitelist instead of wildcard
        # In demo mode, allow all origins; in production, use whitelist
        cors_origins = ["*"] if config.DEMO_MODE else [
            "http://localhost:3000",
            "http://localhost:8000",
            os.getenv("FRONTEND_URL", "http://localhost:3000"),
        ]
        
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=cors_origins,
            allow_credentials=True,
            allow_methods=["GET", "POST", "PUT", "DELETE"],
            allow_headers=["*"],
        )
        
        # Setup routes
        self._setup_routes()
        
        # Setup logging
        self._setup_logging()
        
        # Start background tasks
        self._start_background_tasks()
        
    def _setup_logging(self):
        """Setup logging for SNA."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("SNA")
        
    def _setup_routes(self):
        """Setup API routes."""
        
        @self.app.get("/")
        async def root():
            return {
                "service": "Ghost Protocol - Secure National Aggregator",
                "status": "active",
                "dpdp_compliant": True,
                "sovereignty": "NIC Cloud India",
                "current_round": self.current_round
            }
            
        @self.app.get("/health")
        async def health():
            return {
                "status": "healthy",
                "current_round": self.current_round,
                "pending_updates": len(self.pending_updates),
                "is_aggregating": self.is_aggregating,
                "dpdp_compliant": not self.dpdp_auditor.is_halted
            }
            
        @self.app.post("/submit_update")
        async def submit_update(update_data: Dict[str, Any]):
            """Submit model update from hospital."""
            try:
                hospital_id = update_data.get("hospital_id")
                if not hospital_id:
                    raise HTTPException(status_code=400, detail="Missing hospital_id")
                    
                # Convert weights back to tensors and flatten to update vector
                weights_dict = {}
                update_vector_parts = []
                for name, weight_list in update_data.get("weights", {}).items():
                    tensor = torch.FloatTensor(weight_list)
                    weights_dict[name] = tensor
                    update_vector_parts.append(tensor.flatten())
                
                # Create flattened update vector for Byzantine analysis
                if update_vector_parts:
                    update_vector = torch.cat(update_vector_parts)
                else:
                    update_vector = torch.zeros(1)
                
                # Extract metadata
                metadata = update_data.get("metadata", {})
                dp_compliance = metadata.get("dp_compliance", {})
                
                # Create hospital update with correct fields for new dataclass
                update = HospitalUpdate(
                    hospital_id=hospital_id,
                    update_vector=update_vector,
                    local_auc=metadata.get("local_auc", 0.5),
                    gradient_norm=metadata.get("gradient_norm", 0.0),
                    privacy_budget_spent=dp_compliance.get("epsilon_spent", 0.0),
                    submission_timestamp=datetime.utcnow(),
                    reputation_score=self.byzantine_shield.hospital_reputations.get(hospital_id, 1.0)
                )
                
                # Store weights separately for aggregation
                update.weights = weights_dict
                
                # Add to pending updates
                self.pending_updates.append(update)
                
                # Record privacy expenditure
                if dp_compliance:
                    self.dpdp_auditor.record_privacy_expenditure(
                        hospital_id=hospital_id,
                        round_number=self.current_round,
                        epsilon_spent=dp_compliance.get("epsilon_spent", 0.0),
                        delta_spent=dp_compliance.get("delta", config.DELTA),
                        mechanism=dp_compliance.get("mechanism", "gaussian")
                    )
                
                # Broadcast update via WebSocket (matches frontend handler)
                await self._broadcast_update({
                    "type": "training_update",  # Fixed: matches frontend handler
                    "hospital_id": hospital_id,
                    "round": self.current_round,
                    "auc": metadata.get("local_auc", 0.5),  # Fixed: frontend expects 'auc'
                    "gradient_norm": metadata.get("gradient_norm", 0.0),
                    "epsilon_spent": dp_compliance.get("epsilon_spent", 0.0)
                })
                
                self.logger.info(f"Received update from {hospital_id}: AUC={metadata.get('local_auc', 'N/A')}")
                    
                # Trigger aggregation if enough updates
                if len(self.pending_updates) >= 3:  # Minimum for Byzantine tolerance
                    await self._trigger_aggregation()
                    
                return {
                    "status": "accepted",
                    "hospital_id": hospital_id,
                    "round": self.current_round,
                    "pending_count": len(self.pending_updates),
                    "reputation_score": update.reputation_score
                }
                
            except Exception as e:
                self.logger.error(f"Failed to submit update: {e}")
                import traceback
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=str(e))
                
        @self.app.post("/aggregate")
        async def trigger_aggregation():
            """Manually trigger aggregation."""
            if len(self.pending_updates) < 2:
                return {"status": "insufficient_updates", "pending": len(self.pending_updates)}
                
            await self._trigger_aggregation()
            return {"status": "aggregation_triggered", "round": self.current_round}
            
        @self.app.get("/global_model")
        async def get_global_model():
            """Get current global model weights."""
            # Convert weights to serializable format
            serializable_weights = {}
            for name, weight_tensor in self.global_weights.items():
                serializable_weights[name] = weight_tensor.tolist()
                
            return {
                "round": self.current_round,
                "weights": serializable_weights,
                "performance": self.model_performance,
                "model_info": {
                    "input_size": config.MODEL_INPUT_SIZE,
                    "hidden_size": config.MODEL_HIDDEN_SIZE,
                    "output_size": config.MODEL_OUTPUT_SIZE
                }
            }
            
        @self.app.get("/status")
        async def get_status():
            """Get comprehensive SNA status."""
            return {
                "current_round": self.current_round,
                "model_performance": self.model_performance,
                "pending_updates": len(self.pending_updates),
                "is_aggregating": self.is_aggregating,
                "dpdp_status": self.dpdp_auditor.get_privacy_budget_status(),
                "byzantine_stats": self.byzantine_shield.get_attack_statistics(),
                "health_ledger_stats": self.health_ledger.get_global_statistics(),
                "total_hospitals": len(self.byzantine_shield.hospital_reputations)
            }
            
        @self.app.get("/leaderboard")
        async def get_leaderboard():
            """Get reputation and token leaderboards."""
            reputation_board = self.byzantine_shield.get_reputation_leaderboard()
            token_board = self.health_ledger.get_leaderboard()
            
            return {
                "reputation_leaderboard": reputation_board,
                "token_leaderboard": token_board
            }
            
        @self.app.get("/dpdp_status")
        async def get_dpdp_status():
            """Get DPDP compliance status."""
            return self.dpdp_auditor.get_privacy_budget_status()
            
        @self.app.post("/conduct_audit")
        async def conduct_audit():
            """Conduct privacy audit."""
            report = self.dpdp_auditor.conduct_privacy_audit()
            return asdict(report)
            
        @self.app.websocket("/ws")
        async def websocket_endpoint(websocket: WebSocket):
            """WebSocket for real-time updates with proper cleanup."""
            await websocket.accept()
            self.active_connections.append(websocket)
            
            try:
                while True:
                    # Send status update every 30 seconds
                    await asyncio.sleep(30)
                    status = {
                        "type": "status_update",
                        "round": self.current_round,
                        "pending_updates": len(self.pending_updates),
                        "performance": self.model_performance
                    }
                    # Use timeout to detect dead connections
                    try:
                        await asyncio.wait_for(
                            websocket.send_json(status),
                            timeout=5.0
                        )
                    except asyncio.TimeoutError:
                        self.logger.warning(f"WebSocket send timeout, closing connection")
                        break
                    except Exception as e:
                        self.logger.warning(f"WebSocket send failed: {e}")
                        break
                    
            except WebSocketDisconnect:
                pass  # Normal disconnect
            except Exception as e:
                self.logger.error(f"WebSocket error: {e}")
            finally:
                # Guaranteed cleanup - Fix #8
                if websocket in self.active_connections:
                    self.active_connections.remove(websocket)
                    self.logger.debug("WebSocket connection cleaned up")
                
    def _extract_model_weights(self) -> Dict[str, torch.Tensor]:
        """Extract current model weights."""
        weights = {}
        for name, param in self.global_model.named_parameters():
            weights[name] = param.data.clone().detach()
        return weights
        
    def _set_model_weights(self, weights: Dict[str, torch.Tensor]):
        """Set model weights."""
        for name, param in self.global_model.named_parameters():
            if name in weights:
                param.data = weights[name].clone().detach()
                
    async def _save_checkpoint(self):
        """Save global model checkpoint to disk."""
        try:
            checkpoint_dir = "checkpoints"
            import os
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            filename = f"{checkpoint_dir}/global_model_bucket_{self.current_round}.pt"
            torch.save(self.global_model.state_dict(), filename)
            
            # Also save 'latest'
            torch.save(self.global_model.state_dict(), f"{checkpoint_dir}/global_model_latest.pt")
            
            self.logger.info(f"Saved model checkpoint: {filename}")
        except Exception as e:
            self.logger.error(f"Failed to save checkpoint: {e}")


    async def _trigger_aggregation(self):
        """Trigger model aggregation."""
        if self.is_aggregating or len(self.pending_updates) < 2:
            return
            
        self.is_aggregating = True
        
        try:
            self.logger.info(f"Starting aggregation for round {self.current_round}")
            
            # Extract weights and reputation scores from pending updates
            update_weights_list = []
            reputation_weights = []
            
            for update in self.pending_updates:
                # Get weights dict (stored during submit_update)
                if hasattr(update, 'weights') and update.weights:
                    update_weights_list.append(update.weights)
                    reputation_weights.append(update.reputation_score)
                else:
                    self.logger.warning(f"Skipping update from {update.hospital_id} - no weights")
            
            if len(update_weights_list) < 2:
                self.logger.warning("Not enough valid updates for aggregation")
                return
            
            # Perform Scalable Aggregation (Hierarchical Sharding)
            # This automatically handles splitting workload if N > shard_size
            
            # Prepare updates for the shard aggregator
            prepared_updates = []
            for i, update in enumerate(self.pending_updates):
                if hasattr(update, 'weights') and update.weights:
                    prepared_updates.append({
                        "weights": update.weights,
                        "metadata": {"reputation": update.reputation_score}
                    })
            
            aggregated_weights = await self.shard_aggregator.aggregate_updates(prepared_updates)
            
            # Fix C: Defensive type check - ensure we got a dict, not a coroutine
            if not isinstance(aggregated_weights, dict):
                self.logger.error(f"Unexpected aggregation result type: {type(aggregated_weights)}")
                raise TypeError(f"Expected dict, got {type(aggregated_weights)}")
            
            # Create aggregation report for logging
            aggregation_report = {
                "updates_aggregated": len(update_weights_list),
                "strategy": "geometric_median",
                "anomalies_detected": 0  # Would come from analyze_update calls
            }
            
            # Update global model
            self._set_model_weights(aggregated_weights)
            self.global_weights = aggregated_weights
            
            # Checkpoint the model (persistence)
            await self._save_checkpoint()

            
            # Calculate actual performance improvement based on updates
            avg_local_auc = np.mean([u.local_auc for u in self.pending_updates])
            self.model_performance = min(0.95, max(self.model_performance, avg_local_auc))
            
            # Distribute HealthToken rewards
            round_updates = []
            for update in self.pending_updates:
                round_updates.append({
                    "hospital_id": update.hospital_id,
                    "training_stats": {
                        "val_accuracy": update.local_auc  # Use actual local AUC
                    },
                    "reputation_score": update.reputation_score,
                    "privacy_compliant": True
                })
            
            # Distribute HealthToken rewards using new API (award_tokens for each hospital)
            # Track success/failure for observability (Improvement #4)
            total_tokens_distributed = 0
            rewards_succeeded = 0
            rewards_failed = 0
            
            for update in self.pending_updates:
                try:
                    tokens = await self.health_ledger.award_tokens(
                        hospital_id=update.hospital_id,
                        round_id=str(self.current_round),
                        local_auc=update.local_auc,
                        reputation_score=update.reputation_score,
                        gradient_quality=1.0 - min(update.gradient_norm / 10.0, 1.0),  # Higher quality = lower norm
                        participation_history=[]  # Would be populated from hospital history
                    )
                    total_tokens_distributed += float(tokens)
                    rewards_succeeded += 1
                except Exception as e:
                    # Downgrade to WARNING (Improvement #3) - expected in mock chain mode
                    self.logger.warning(f"Token award pending for {update.hospital_id}: {e}")
                    rewards_failed += 1
            
            # Emit reward summary (Improvement #4)
            self.logger.info(
                f"Reward distribution: attempted={rewards_succeeded + rewards_failed}, "
                f"succeeded={rewards_succeeded}, failed={rewards_failed}, "
                f"tokens_distributed={total_tokens_distributed:.2f}"
            )
            
            # Create reward report
            reward_report = {
                "total_distributed": total_tokens_distributed,
                "succeeded": rewards_succeeded,
                "failed": rewards_failed
            }
            
            # Clear pending updates
            self.pending_updates = []
            
            # Increment round
            self.current_round += 1
            
            # Record in history
            self.round_history.append({
                "round": self.current_round - 1,
                "performance": self.model_performance,
                "hospitals": len(round_updates),
                "anomalies": aggregation_report.get("anomalies_detected", 0),
                "rewards_distributed": reward_report.get("total_distributed", 0)
            })
            
            # Enhanced aggregation metrics (Improvement #5)
            if round_updates:
                aucs = [u.local_auc for u in self.pending_updates] if hasattr(self, '_last_updates') else []
                if not aucs and round_updates:
                    aucs = [ru.get("training_stats", {}).get("val_accuracy", 0) for ru in round_updates]
                
                min_auc = min(aucs) if aucs else 0
                max_auc = max(aucs) if aucs else 0
                variance = np.var(aucs) if aucs else 0
                
                self.logger.info(
                    f"Aggregation completed: round={self.current_round-1}, "
                    f"performance={self.model_performance:.3f}, hospitals={len(round_updates)}, "
                    f"AUC_range=[{min_auc:.3f}, {max_auc:.3f}], variance={variance:.4f}"
                )
            else:
                self.logger.info(
                    f"Aggregation completed: round={self.current_round-1}, "
                    f"performance={self.model_performance:.3f}, "
                    f"hospitals={len(round_updates)}"
                )
            
            # Broadcast update to WebSocket clients
            await self._broadcast_update({
                "type": "aggregation_complete",
                "round": self.current_round - 1,
                "performance": self.model_performance,
                "hospitals": len(round_updates),
                "accepted": len(round_updates),  # All updates that passed validation
                "total": len(round_updates)       # Total submitted (before filtering)
            })
            
        except Exception as e:
            self.logger.error(f"Aggregation failed: {e}")
            raise
        finally:
            self.is_aggregating = False
            
    async def _broadcast_update(self, message: Dict[str, Any]):
        """Broadcast message to all WebSocket connections."""
        disconnected = []
        
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception:
                disconnected.append(connection)
                
        # Remove disconnected clients
        for conn in disconnected:
            self.active_connections.remove(conn)
            
    def _start_background_tasks(self):
        """Start background tasks."""
        # Note: Aggregation is triggered automatically in submit_update when 
        # enough updates are received (>= 3). No need for separate timer thread.
        # This avoids the threading + asyncio issues.
        self.logger.info("Background tasks initialized - aggregation triggers on update threshold")
    
    async def graceful_shutdown(self):
        """
        Graceful shutdown handler - Fix #15.
        
        Performs:
        1. Stop accepting new updates
        2. Complete pending aggregation
        3. Save final checkpoint
        4. Close WebSocket connections
        5. Persist final state
        6. Close database connections
        7. Close cache connections
        """
        if self.is_shutting_down:
            return
        
        self.is_shutting_down = True
        self.accepting_updates = False
        self.logger.info("Starting graceful shutdown...")
        
        try:
            # 1. Complete pending aggregation if any
            if self.pending_updates and not self.is_aggregating:
                self.logger.info(f"Completing {len(self.pending_updates)} pending updates before shutdown...")
                try:
                    await asyncio.wait_for(
                        self._trigger_aggregation(),
                        timeout=30.0  # Give 30s max for aggregation
                    )
                except asyncio.TimeoutError:
                    self.logger.warning("Aggregation timeout during shutdown, proceeding...")
                except Exception as e:
                    self.logger.error(f"Aggregation error during shutdown: {e}")
            
            # 2. Save final checkpoint
            self.logger.info("Saving final checkpoint...")
            await self._save_checkpoint()
            
            # 3. Persist final round to database
            if self.persistence.is_available:
                await self.persistence.save_round(
                    round_number=self.current_round,
                    weights=self.global_weights,
                    performance=self.model_performance,
                    hospitals=len(self.pending_updates),
                    metadata={"shutdown": True, "timestamp": datetime.utcnow().isoformat()}
                )
                self.logger.info("Final state persisted to database")
            
            # 4. Close WebSocket connections gracefully
            self.logger.info(f"Closing {len(self.active_connections)} WebSocket connections...")
            shutdown_message = {
                "type": "server_shutdown",
                "message": "Server shutting down gracefully",
                "final_round": self.current_round
            }
            
            for ws in self.active_connections.copy():
                try:
                    await asyncio.wait_for(
                        ws.send_json(shutdown_message),
                        timeout=2.0
                    )
                    await ws.close(code=1001, reason="Server shutting down")
                except Exception:
                    pass  # Best effort
            
            self.active_connections.clear()
            
            # 5. Close cache connections
            await self.cache.close()
            self.logger.info("Cache connections closed")
            
            # 6. Close database connections
            await self.persistence.close()
            self.logger.info("Database connections closed")
            
            self.logger.info("Graceful shutdown complete")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
        
    def run(self, host: str = "0.0.0.0", port: Optional[int] = None):
        """Run the SNA with graceful shutdown support."""
        port = port or self.sna_port
        
        self.logger.info(f"Starting Secure National Aggregator on {host}:{port}")
        
        # Setup async shutdown handler
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # Setup signal handlers for graceful shutdown
        def signal_handler(sig, frame):
            self.logger.info(f"Received signal {sig}, initiating graceful shutdown...")
            if not self.is_shutting_down:
                loop.run_until_complete(self.graceful_shutdown())
            sys.exit(0)
            
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Initialize async components before running server
        async def startup():
            # Connect cache
            await self.cache.connect()
            self.logger.info("Cache initialized")
            
            # Initialize persistence
            await self.persistence.initialize()
            
            # Try to recover previous state
            latest = await self.persistence.load_latest_round()
            if latest:
                self.current_round = latest["round_number"]
                self.global_weights = latest["weights"]
                self.model_performance = latest["performance"]
                self._set_model_weights(self.global_weights)
                self.logger.info(f"Recovered state from round {self.current_round}")
        
        # Register startup event
        @self.app.on_event("startup")
        async def on_startup():
            await startup()
        
        @self.app.on_event("shutdown")
        async def on_shutdown():
            await self.graceful_shutdown()
        
        # Run server
        uvicorn.run(self.app, host=host, port=port)


def main():
    """Main entry point for SNA."""
    # Get configuration from environment
    sna_port = int(os.getenv("SNA_PORT", "8000"))
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    vault_addr = os.getenv("VAULT_ADDR", "http://localhost:8200")
    
    # Create and run SNA
    sna = SecureNationalAggregator(
        sna_port=sna_port,
        redis_url=redis_url,
        vault_addr=vault_addr
    )
    
    # Run SNA
    sna.run()


if __name__ == "__main__":
    main()